---
services:
  load-cp-images:
    restart: "no"
    expose:
      - "8080"
    build:
      context: ./sandbox/load_cp_images
    networks:
      - crs-internal
      - internet
    env_file: ./config/env
    environment:
      - DOCKER_HOST=tcp://dind:2375
      - CP_ROOT=/cp_root
      - "LOAD_CPS=false"
      - "LOAD_CP_IMAGES=true"
    volumes:
      - type: bind
        source: ${PWD}/cp_root
        target: /cp_root
        bind:
          propagation: rshared
    depends_on:
      dind:
        condition: service_healthy
  dind:
    networks:
      - crs-internal
      - internet
    expose:
      - "2375"
    image: docker:24-dind
    command: ["dockerd", "-H", "tcp://0.0.0.0:2375", "--tls=false", "--storage-driver=overlay2"]
    restart: always
    privileged: true  # This must run with privilege to support nested virtualization within the public Linux CP for `virtme-ng`
    environment:
      - DOCKER_TLS_CERTDIR  # intentionally blank to optimize runtime
    healthcheck:
      test: ["CMD", "docker", "-H", "tcp://0.0.0.0:2375", "version"]
      start_period: 10s
      start_interval: 5s
      interval: 1m30s
    volumes:
      - type: bind
        source: ${PWD}/crs_scratch
        target: /crs_scratch
        bind:
          propagation: rshared
      - dind_cache:/var/lib/docker
  db:
    networks:
      - crs-internal
    expose:
      - "5432"
    image: postgres:16.2-alpine3.19
    restart: always
    shm_size: 128mb
    environment:
      - POSTGRES_PASSWORD=aixcc
      - POSTGRES_USER=aixcc
      - POSTGRES_DB=litellm
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}"]
      interval: 10s
      timeout: 3s
      retries: 15
  litellm:
    networks:
      - crs-internal
      - internet
    expose:
      - "80"
    ports:
      - '127.0.0.1:8081:80'
    image: ghcr.io/berriai/litellm-database:litellm_stable_release_branch-v1.57.8-stable
    restart: always
    configs:
      - source: litellm_config
        target: /app/config.yaml
      - source: litellm_vertex_config
        target: /vertex_key.json
    command: ["--config", "/app/config.yaml", "--port", "80", "--num_workers", "8"]
    env_file: ./config/env
    environment:
      - DATABASE_URL=postgresql://aixcc:aixcc@db:5432/litellm
    healthcheck:
      test: ["CMD-SHELL", "python3", "--version"]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 1s
    depends_on:
      db:
        condition: service_healthy
  crs:
    restart: "no"
    build:
      context: ./sandbox/crs/
      dockerfile: Dockerfile
    volumes:
      - type: bind
        source: ${PWD}/crs_scratch
        target: /crs_scratch
        bind:
          propagation: rshared
      - type: bind
        source: ${PWD}/sandbox/crs/src/
        target: /app/
        bind:
          propagation: rshared
      - ./cp_root:/cp_root
    environment:
      - DOCKER_HOST=tcp://dind:2375
      - LITELLM_HOSTNAME=http://litellm
      - CP_ROOT=/cp_root
      - CRS_SCRATCH_SPACE=/crs_scratch
      - LITELLM_KEY=sk-1234
    depends_on:
      litellm:
        condition: service_healthy
      load-cp-images:
        condition: service_completed_successfully
    # command: ["tail", "-f", "run.py"]  # useful to keep CRS container alive in conjunction with `make crs-shell`
    networks:
      - crs-internal
      - internet  # only to be used for langsmith
    env_file:
      - path: ./config/crs.env
        required: false
volumes:
  dind_cache:
configs:
  litellm_config:
    file: ./config/local_litellm_config.yaml
  litellm_vertex_config:
    file: ./config/vertex_key.json
networks:
  internet: {}
  crs-internal:
    internal: true
